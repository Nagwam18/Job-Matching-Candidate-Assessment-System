{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13879561,"sourceType":"datasetVersion","datasetId":8842878},{"sourceId":13879935,"sourceType":"datasetVersion","datasetId":8843112},{"sourceId":13912823,"sourceType":"datasetVersion","datasetId":8864990},{"sourceId":13913555,"sourceType":"datasetVersion","datasetId":8865490},{"sourceId":14087637,"sourceType":"datasetVersion","datasetId":8969710}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Job-Matching-Candidate-Assessment-System\n An end-to-end AI-driven recruitment system that evaluates candidates using multi-modal analysis:\n\n* CV & Job Description matching using **Large Language Models (LLMs)**\n* Skill extraction and **quantitative scoring**\n* Video-based **emotion and behavior** analysis\n* Final **HR-style** verdict generation\n* **You Can Check Project GitHub Repo From [this link](https://github.com/Nagwam18/Job-Matching-Candidate-Assessment-System)**","metadata":{}},{"cell_type":"markdown","source":"# Installation","metadata":{}},{"cell_type":"code","source":"!pip cache purge\n!rm -rf ~/.cache/pip\n!rm -rf ~/.cache/*\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -r /kaggle/input/new1-requirements/requirements.txt\n!pip install pdfplumber","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport io\nimport re\nimport uuid\nimport json\nimport tempfile\n\nfrom fastapi import FastAPI, UploadFile, File, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom starlette.responses import JSONResponse\n\nimport pdfplumber\nimport pytesseract\nimport docx2txt\nimport re\nfrom PIL import Image\n\nimport cv2\nfrom deepface import DeepFace\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport cv2\nfrom collections import Counter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"markdown","source":"## load_file","metadata":{}},{"cell_type":"code","source":"def load_file(path):\n    ext = os.path.splitext(path)[1].lower()\n    if ext==\".pdf\":\n        text=\"\"\n        with pdfplumber.open(path) as pdf:\n            for p in pdf.pages:\n                t = p.extract_text()\n                if t: text+=t+\"\\n\"\n        if text.strip():\n            return text\n    \n        ocr=\"\"\n        with pdfplumber.open(path) as pdf:\n            for p in pdf.pages:\n                img=p.to_image(resolution=300).original\n                ocr+=pytesseract.image_to_string(img)\n        return ocr\n    if ext==\".docx\":\n        return docx2txt.process(path) or \"\"\n    if ext==\".txt\":\n        return open(path,\"r\",encoding=\"utf-8\", errors=\"ignore\").read()\n    if ext in [\".jpg\",\".jpeg\",\".png\"]:\n        return pytesseract.image_to_string(Image.open(path))\n    return \"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    cache_dir=\"/kaggle/temp_model\",\n    trust_remote_code=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    cache_dir=\"/kaggle/temp_model\",\n    device_map=\"auto\",         \n    torch_dtype=torch.float16, \n    \n    trust_remote_code=True,\n    repetition_penalty=1.15     )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## analyze_cv","metadata":{}},{"cell_type":"code","source":"import re\nimport torch\n\ndef analyze_cv(cv_text, jd_text):\n    prompt = f\"\"\"\nYou are an expert recruiter.\n\nCV:\n{cv_text}\n\nJob Description:\n{jd_text}\n\nInstructions:\n1. Identify all skills required in the Job Description.\n2. Identify all skills mentioned in the CV.\n3. Extract the matched skills (skills present in both CV and JD, considering synonyms and related concepts).\n4. Extract the non-matched skills (skills required in JD but not present in CV).\n5. Calculate CV Score = (number of matched skills / total skills in JD) * 100, rounded to 1 decimal.\n\nReturn ONLY in this exact format:\n\nMatched Skills\n- skill\n\nNon-Matched Skills\n- skill\n\nCV Score\nXX.X% match\n\nRules:\n- Do not repeat any skills.\n- Do not use placeholders.\n- Do not explain anything.\n- Output must end after CV Score.\n\"\"\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=1024,\n        temperature=0.0,\n        do_sample=False,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    if \"Matched Skills\" in output_text:\n        output_text = output_text.split(\"Matched Skills\", 1)[1]\n        output_text = \"Matched Skills\\n\" + output_text\n\n    return output_text.strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## extract_cv score&skills","metadata":{}},{"cell_type":"code","source":"def extract_skills(cv_output: str):\n    matched, missing = [], []\n\n    matched_section = re.search(\n        r\"Matched Skills\\s*:?(.*?)(Non-Matched Skills)\",\n        cv_output,\n        re.S | re.I\n    )\n\n    if matched_section:\n        for line in matched_section.group(1).splitlines():\n            skill = line.strip(\"- \").strip()\n            if skill:\n                matched.append(skill)\n\n    missing_section = re.search(\n        r\"Non-Matched Skills\\s*:?(.*?)(CV Score)\",\n        cv_output,\n        re.S | re.I\n    )\n\n    if missing_section:\n        for line in missing_section.group(1).splitlines():\n            skill = line.strip(\"- \").strip()\n            if skill:\n                missing.append(skill)\n\n    return matched, missing\n\ndef extract_cv_score(cv_output: str):\n    score_match = re.search(\n        r\"CV Score\\s*([\\d\\.]+)\\s*%\",\n        cv_output,\n        re.I\n    )\n    return float(score_match.group(1)) if score_match else None\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path=\"/kaggle/input/cv-version3/Nagwa Mohamed - AI - Machine Learning Engineer-1.pdf\"\njd=\"\"\"\n\nWe are looking for a Generative AI Engineer to join our research and development team.\nResponsibilities:\n- Design and fine-tune large language models (LLMs) using frameworks such as HuggingFace Transformers.\n- Build and deploy generative AI applications for text, image, and multimodal tasks.\n- Implement training pipelines with PyTorch or TensorFlow, and optimize models for performance.\n- Work with cloud platforms (AWS, Azure, GCP) to scale model training and deployment.\n- Apply MLOps practices using Docker, Kubernetes, MLflow, and Airflow.\n- Collaborate with data scientists to integrate generative models into production systems.\n- Research and experiment with diffusion models, GANs, and reinforcement learning for generative tasks.\n\nRequirements:\n- Strong programming skills in Python.\n- Experience with deep learning architectures (CNNs, RNNs, Transformers).\n- Knowledge of NLP techniques and computer vision.\n- Familiarity with CI/CD pipelines and GitHub Actions.\n- Bonus skills: LangChain, RAG (Retrieval-Augmented Generation), Vector Databases (FAISS, Pinecone, Weaviate), Databricks.\n\n\n\"\"\"\n\ncv_text = load_file(path)\ncv_output = analyze_cv(cv_text, jd)\n\nmatched_skills, non_matched_skills = extract_skills(cv_output)\ncv_score = extract_cv_score(cv_output)\n\nresult = {\n    \"matched_skills\": matched_skills,\n    \"non_matched_skills\": non_matched_skills,\n    \"cv_score\": cv_score\n}\n\nprint(result)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## analyze_video","metadata":{}},{"cell_type":"code","source":"import cv2\nfrom deepface import DeepFace\nfrom collections import Counter\n\ndef analyze_video(video_path):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    frames_samples = 20\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        mean_intensity = gray.mean()\n        if mean_intensity < 10:  \n            continue        \n        frames.append(frame)\n\n    cap.release()\n\n    if len(frames) == 0:\n        return {\"confidence\": 0, \"anxiety\": 0, \"emotions_distribution\": {}, \"frames_sampled\": 0}\n\n    step = max(1, len(frames) // frames_samples)\n    selected_frames = [frames[i] for i in range(0, len(frames), step)][:frames_samples]\n\n    emotions = []\n    for frame in selected_frames:\n        try:\n            result = DeepFace.analyze(frame, actions=[\"emotion\"], enforce_detection=False)\n            if isinstance(result, list):\n                result = result[0]\n            emotions.append(result.get(\"dominant_emotion\", \"unknown\"))\n        except Exception:\n            emotions.append(\"unknown\")\n\n    confidence_emotions = {\"happy\", \"neutral\", \"calm\"}\n    anxiety_emotions = {\"fear\", \"sad\", \"angry\", \"disgust\"}     \n\n    distribution = Counter(emotions)\n\n    confidence_count = sum(distribution[e] for e in confidence_emotions)\n    anxiety_count = sum(distribution[e] for e in anxiety_emotions)\n    total = confidence_count + anxiety_count\n\n    confidence_score = round((confidence_count / total) * 100, 1) if total > 0 else 0.0\n    anxiety_score = round((anxiety_count / total) * 100, 1) if total > 0 else 0.0\n\n    return {\n        \"confidence\": confidence_score,   \n        \"anxiety\": anxiety_score,         \n        \"emotions_distribution\": dict(distribution),\n        \"frames_sampled\": len(selected_frames)\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# video_path=\"/kaggle/input/test2v/test2.mp4\"\n# result = analyze_video(video_path)\n# print(result)\n# print(result[\"confidence\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# video_path=\"/kaggle/input/test2v/test2.mp4\"\n# result = analyze_video(video_path)\n# print(result)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## generate_verdict","metadata":{}},{"cell_type":"code","source":"import re\n\ndef generate_verdict(cv_score, matched, missing, confidence, anxiety):\n    cv_score = cv_score or 0.0\n    confidence = confidence or 0.0\n    anxiety = anxiety or 0.0\n    matched = matched or []\n    missing = missing or []\n\n    behavior = (\n        \"confident\" if confidence > anxiety else\n        \"anxious\" if anxiety > confidence else\n        \"neutral\"\n    )\n\n    video_score = confidence\n    final_score = round((cv_score * 0.7 + video_score * 0.3), 2)\n\n    if cv_score == 0.0 or not matched:\n        verdict_text = (\n            f\"The candidate has no matching skills in the CV and appears {behavior}. \"\n            \"The candidate is not suitable for the position.\"\n        )\n        return verdict_text, final_score, behavior, video_score\n\n    matched_skills = \", \".join(matched)\n    missing_skills = \", \".join(missing) if missing else \"None\"\n\n    prompt = f\"\"\"\nYou are an expert recruiter.\n\nCandidate Evaluation:\n\n- CV Score: {cv_score}%\n- Video Behavior: {behavior}\n- Matched Skills: {matched_skills}\n- Missing Skills: {missing_skills}\n- Final Score: {final_score}\n\nTask:\n\nWrite a concise HR verdict in EXACTLY TWO sentences. \nSentence 1: Highlight the candidate's strengths and matched skills.\nSentence 2: Mention missing skills and video behavior, then give a clear recommendation.\nDo not add anything else.\n\"\"\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=256,\n        temperature=0.3,\n        top_p=0.9,\n        do_sample=True\n    )\n\n    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    if full_output.startswith(prompt):\n        verdict_text = full_output[len(prompt):].strip()\n    else:\n        verdict_text = full_output.strip()\n\n    sentences = re.split(r'(?<=[.!?])\\s+', verdict_text)\n\n    if len(sentences) >= 2:\n        verdict_text = ' '.join(sentences[:2])\n    elif len(sentences) == 1:\n        verdict_text = sentences[0]\n    else:\n        verdict_text = (\n            f\"The candidate shows {behavior} behavior with CV score {cv_score}%. \"\n            f\"Matched skills: {matched_skills}. Missing skills: {missing_skills}.\"\n        )\n\n    return verdict_text, final_score, behavior, video_score\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_output(cv_score, video_score, final_score, verdict, behavior):\n    text = f\"\"\"Final Score:\n• CV Score (70%): {cv_score}\n• Video Score (30%): {video_score}\n• Candidate behavior: {behavior}\n• Final = {final_score}\n{verdict}\n\"\"\"\n    return text\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FASTAPI APP","metadata":{}},{"cell_type":"code","source":"# !pip install fastapi uvicorn nest_asyncio python-multipart\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import uvicorn\nfrom fastapi import FastAPI, UploadFile, File, Form\nfrom threading import Thread\nimport uuid\nimport os\nimport requests\nfrom threading import Thread\nimport json\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nest_asyncio\nnest_asyncio.apply()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"app = FastAPI()\nresults = {}\n\n@app.post(\"/evaluate\")\nasync def evaluate(\n    job_description: str = Form(...),\n    cv_path: str = Form(...),\n    video_path: str = Form(...)\n):\n    session_id = str(uuid.uuid4())\n\n    cv_text = load_file(cv_path)\n    cv_analysis = analyze_cv(cv_text, job_description)\n    cv_score = extract_cv_score(cv_analysis) or 0.0 \n    matched, missing = extract_skills(cv_analysis)\n    \n    video_result = analyze_video(video_path)\n    confidence = video_result.get(\"confidence\", 0.0) \n    anxiety = video_result.get(\"anxiety\", 0.0)\n    \n    verdict, final_score, behavior, video_score = generate_verdict(\n        cv_score, matched, missing, confidence, anxiety\n    )\n\n\n    formatted_output = format_output(cv_score, video_score, final_score, verdict, behavior)\n\n    results[session_id] = {\n        \"cv_text\": cv_text,\n        \"cv_analysis\": cv_analysis,\n        \"cv_score\": cv_score,\n        \"matched_skills\": matched,\n        \"missing_skills\": missing,\n        \"video_result\": video_result,\n        \"behavior\": behavior,\n        \"final_score\": final_score,\n        \"llm_verdict\": verdict,\n        \"formatted_output\": formatted_output  \n    }\n\n    return {\"session_id\": session_id, \"output\": formatted_output}\n\n\n\n@app.get(\"/results\")\nasync def get_results(session_id: str):\n    if session_id not in results:\n        return {\"error\": \"invalid session id\"}\n    return results[session_id]\n\n\ndef run_api():\n  uvicorn.run(app, host=\"0.0.0.0\", port=8009)\n\nthread = Thread(target=run_api, daemon=True)\nthread.start()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"jd=\"\"\"To support all aspects of human resources functions within the company, including recruitment, training and development, performance management, and ensuring compliance with company policies and procedures to maintain an effective and organized work environment.\n\nKey Responsibilities:\n\nManage recruitment processes: posting job openings, screening resumes, conducting interviews, and selecting candidates.\n\nCoordinate employee training and development programs.\n\nMonitor performance evaluations and contribute to employee development plans.\n\nImplement company policies and procedures and ensure compliance.\n\nMaintain and update employee records.\n\nHandle employee-related matters such as leave, attendance, and benefits.\n\nProvide support and guidance to employees regarding internal policies and company regulations.\n\nPrepare HR reports and statistics as needed.\n\nQualifications:\n\nBachelor’s degree in Business Administration, Human Resources, or related field.\n\n1–3 years of experience in Human Resources (depending on level).\n\nGood knowledge of labor laws and local regulations.\n\nStrong communication and organizational skills.\n\nAbility to handle sensitive information with confidentiality.\n\nProficiency in computer applications and HR software.\n\nPersonal Skills:\n\nProblem-solving and decision-making abilities.\n\nNegotiation and persuasion skills.\n\nAbility to work collaboratively in a team environment.\n\n\n\"\"\"\n\nresp = requests.post(\n    \"http://127.0.0.1:8009/evaluate\",\n    data={\n        \"job_description\": jd,\n        \"cv_path\": \"/kaggle/input/cv-version4/Nagwa Mohamed-Data Scientist II.pdf\",\n        \"video_path\": \"/kaggle/input/video1/test1.webm\"\n    }\n)\n\ndata = resp.json()  \nprint(data[\"output\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dash App","metadata":{}},{"cell_type":"code","source":"from pyngrok import ngrok, conf\nconf.get_default().auth_token = \"36F0BxzfgoiXChAeN7oJ4MflnlF_2AjXn8jbAnkxHQh8WLAiT\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import dash\nfrom dash import html, dcc, Input, Output, State\nimport dash_bootstrap_components as dbc\nimport base64\n\napp = dash.Dash(\n    __name__,\n    external_stylesheets=[dbc.themes.BOOTSTRAP],\n    suppress_callback_exceptions=True\n)\n\n# LAYOUT\napp.layout = html.Div(\n    style={\"padding\": \"30px\", \"backgroundColor\": \"#f5f7fa\"},\n    children=[\n\n        html.Div(\n            style={\"display\": \"flex\", \"gap\": \"30px\", \"marginTop\": \"20px\"},\n            children=[\n\n                # LEFT SIDE (UPLOADS)\n                html.Div(\n                    style={\"width\": \"48%\", \"background\": \"white\",\n                           \"padding\": \"20px\", \"borderRadius\": \"10px\"},\n                    children=[\n\n                        html.H4(\"Upload CV\"),\n                        dcc.Upload(\n                            id=\"upload-cv\",\n                            children=html.Div(\n                                id=\"cv-upload-text\",\n                                children=[\"Drag & Drop or Select CV File\"]\n                            ),\n                            style={\n                                \"height\": \"120px\",\n                                \"border\": \"2px dashed #007bff\",\n                                \"borderRadius\": \"10px\",\n                                \"textAlign\": \"center\",\n                                \"paddingTop\": \"40px\",\n                                \"color\": \"#007bff\"\n                            }\n                        ),\n                        html.Div(id=\"cv_upload_status\", style={\"marginTop\": \"10px\"}),\n\n                        html.H4(\"Upload Video\", style={\"marginTop\": \"30px\"}),\n                        dcc.Upload(\n                            id=\"upload-video\",\n                            children=html.Div(\n                                id=\"video-upload-text\",\n                                children=[\"Drag & Drop or Select Video File\"]\n                            ),\n                            style={\n                                \"height\": \"120px\",\n                                \"border\": \"2px dashed #28a745\",\n                                \"borderRadius\": \"10px\",\n                                \"textAlign\": \"center\",\n                                \"paddingTop\": \"40px\",\n                                \"color\": \"#28a745\"\n                            }\n                        ),\n                        html.Div(id=\"video_upload_status\", style={\"marginTop\": \"10px\"}),\n                    ]\n                ),\n\n                # RIGHT SIDE (JD)\n                html.Div(\n                    style={\"width\": \"48%\", \"background\": \"white\",\n                           \"padding\": \"20px\", \"borderRadius\": \"10px\"},\n                    children=[\n                        html.H4(\"Job Description\"),\n                        dcc.Textarea(\n                            id=\"jd-text\",\n                            placeholder=\"Paste Job Description here...\",\n                            style={\"width\": \"100%\", \"height\": \"300px\",\n                                   \"borderRadius\": \"10px\",\n                                   \"padding\": \"10px\",\n                                   \"border\": \"1px solid #ccc\"}\n                        ),\n                    ]\n                )\n            ]\n        ),\n\n        # RESULTS\n        html.Div(\n            style={\"background\": \"white\", \"padding\": \"20px\",\n                   \"marginTop\": \"30px\", \"borderRadius\": \"10px\"},\n            children=[\n\n                html.Div(style={\"textAlign\": \"center\"}, children=[\n                    html.Button(\n                        \"Evaluate Candidate\",\n                        id=\"eval-btn\",\n                        n_clicks=0,\n                        style={\n                            \"background\": \"#007bff\",\n                            \"color\": \"white\",\n                            \"padding\": \"14px 20px\",\n                            \"borderRadius\": \"8px\",\n                            \"fontSize\": \"25px\",\n                            \"cursor\": \"pointer\"\n                        }\n                    )\n                ]),\n\n                dcc.Loading(\n                    id=\"loading_wrapper\",\n                    type=\"circle\",\n                    color=\"#003366\",\n                    children=html.Div(\n                        id=\"results-output\",\n                        style={\n                            \"whiteSpace\": \"pre-wrap\",\n                            \"minHeight\": \"150px\",\n                            \"marginTop\": \"20px\",\n                            \"fontSize\": \"17px\"\n                        }\n                    )\n                )\n            ]\n        )\n    ]\n)\n\n#CV UPLOAD STATUS CALLBACK\n@app.callback(\n    Output(\"cv-upload-text\", \"children\"),\n    Input(\"upload-cv\", \"contents\"),\n    State(\"upload-cv\", \"filename\"),\n)\ndef update_cv_status(content, filename):\n    if content:\n        return f\"Uploaded: {filename}\"\n    return \"Drag & Drop or Select CV File\"\n\n# VIDEO UPLOAD STATUS CALLBACK\n@app.callback(\n    Output(\"video-upload-text\", \"children\"),\n    Input(\"upload-video\", \"contents\"),\n    State(\"upload-video\", \"filename\"),\n)\ndef update_video_status(content, filename):\n    if content:\n        return f\"Uploaded: {filename}\"\n    return \"Drag & Drop or Select Video File\"\n\n\n#MAIN EVALUATION CALLBACK\n@app.callback(\n    Output(\"results-output\", \"children\"),\n    Input(\"eval-btn\", \"n_clicks\"),\n    State(\"upload-cv\", \"contents\"),\n    State(\"upload-cv\", \"filename\"),\n    State(\"upload-video\", \"contents\"),\n    State(\"upload-video\", \"filename\"),\n    State(\"jd-text\", \"value\"),\n)\ndef evaluate(n_clicks, cv_content, cv_name, video_content, video_name, jd_text):\n\n    if not n_clicks:\n        return \"\"\n\n    if not (cv_content and video_content and jd_text):\n        return \"Please upload CV, Video, and paste JD.\"\n\n    # SAVE CV \n    cv_data = cv_content.split(\",\")[1]\n    cv_bytes = base64.b64decode(cv_data)\n\n    cv_path = f\"/tmp/{cv_name}\"\n    with open(cv_path, \"wb\") as f:\n        f.write(cv_bytes)\n\n    cv_text = load_file(cv_path)\n    result= analyze_cv(cv_text, jd_text)\n    cv_score = extract_cv_score(result)\n    matched, missing = extract_skills(result)\n\n\n    #SAVE VIDEO \n    video_data = video_content.split(\",\")[1]\n    video_bytes = base64.b64decode(video_data)\n\n    video_path = f\"/tmp/{video_name}\"\n    with open(video_path, \"wb\") as f:\n        f.write(video_bytes)\n\n    video_result = analyze_video(video_path)\n    confidence = video_result.get(\"confidence\", 0.0)\n    anxiety = video_result.get(\"anxiety\", 0.0)\n\n    verdict, final_score, behavior, video_score = generate_verdict(\n    cv_score, matched, missing, confidence, anxiety  )\n                                \n\n    result_text = format_output(cv_score, video_score, final_score, verdict, behavior)\n\n    return html.Pre(result_text, style={\"whiteSpace\": \"pre-wrap\", \"fontSize\": \"25px\"})\n\n\nif __name__ == \"__main__\":\n    from pyngrok import ngrok\n    ngrok.kill()\n    public_url = ngrok.connect(8050)\n    print(\"Dash app running at:\", public_url)\n    app.run(host=\"0.0.0.0\", port=8050)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}